{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87eacae1",
   "metadata": {},
   "source": [
    "### Notebook Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbff59db",
   "metadata": {},
   "source": [
    "In this notebook, we use PySpark on EMR (via JupyterHub) to read the Google Books Ngrams CSV that was copied into HDFS. We validate the load by inspecting the schema and reporting row and column counts, as well as some other basic info. Next, we run a Spark SQL query to filter rows where token = \"data\" and briefly examine the filtered dataset. Finally, we write the filtered results back to HDFS as a CSV, verify the output, and prepare for the downstream step of merging/uploading to S3 for local analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b069519",
   "metadata": {},
   "source": [
    "### Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8787ebf9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Imports\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eab32a75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Check that Spark session exists\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "60cf0f32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Read csv from HDFS\n",
    "\n",
    "PATH = f\"hdfs:///user/hadoop/eng_1M_1gram/eng_1M_1gram.csv\"\n",
    "\n",
    "ngram_df = (spark.read\n",
    "           .option(\"header\",True)\n",
    "           .csv(PATH, inferSchema=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "56d19bfa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[token: string, year: int, frequency: int, pages: int, books: int]"
     ]
    }
   ],
   "source": [
    "# Cache the dataframe to speed up repeated operations\n",
    "\n",
    "ngram_df.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d7ee2101",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe type is <class 'pyspark.sql.dataframe.DataFrame'>."
     ]
    }
   ],
   "source": [
    "# Confirm this is a Spark dataframe\n",
    "\n",
    "print(f\"Dataframe type is {type(ngram_df)}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "232bb722",
   "metadata": {},
   "source": [
    "### Inspect data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "495b3738",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of rows is 261823225. The number of columns is 5."
     ]
    }
   ],
   "source": [
    "# Count the number of rows and columns\n",
    "\n",
    "rows = ngram_df.count()\n",
    "cols = len(ngram_df.columns)\n",
    "\n",
    "\n",
    "print(f\"The number of rows is {rows}. The number of columns is {cols}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "412e75eb",
   "metadata": {},
   "source": [
    "This is a massive dataset with 261,823,225 rows. However, the column count is just 5. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f594ed15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- token: string (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- frequency: integer (nullable = true)\n",
      " |-- pages: integer (nullable = true)\n",
      " |-- books: integer (nullable = true)"
     ]
    }
   ],
   "source": [
    "# Examine schema\n",
    "\n",
    "ngram_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "013746e4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0--------------\n",
      " token     | inGermany \n",
      " year      | 1927      \n",
      " frequency | 2         \n",
      " pages     | 2         \n",
      " books     | 2         \n",
      "-RECORD 1--------------\n",
      " token     | inGermany \n",
      " year      | 1929      \n",
      " frequency | 1         \n",
      " pages     | 1         \n",
      " books     | 1         \n",
      "-RECORD 2--------------\n",
      " token     | inGermany \n",
      " year      | 1930      \n",
      " frequency | 1         \n",
      " pages     | 1         \n",
      " books     | 1         \n",
      "-RECORD 3--------------\n",
      " token     | inGermany \n",
      " year      | 1933      \n",
      " frequency | 1         \n",
      " pages     | 1         \n",
      " books     | 1         \n",
      "-RECORD 4--------------\n",
      " token     | inGermany \n",
      " year      | 1934      \n",
      " frequency | 1         \n",
      " pages     | 1         \n",
      " books     | 1         \n",
      "-RECORD 5--------------\n",
      " token     | inGermany \n",
      " year      | 1935      \n",
      " frequency | 1         \n",
      " pages     | 1         \n",
      " books     | 1         \n",
      "-RECORD 6--------------\n",
      " token     | inGermany \n",
      " year      | 1938      \n",
      " frequency | 5         \n",
      " pages     | 5         \n",
      " books     | 5         \n",
      "-RECORD 7--------------\n",
      " token     | inGermany \n",
      " year      | 1939      \n",
      " frequency | 1         \n",
      " pages     | 1         \n",
      " books     | 1         \n",
      "-RECORD 8--------------\n",
      " token     | inGermany \n",
      " year      | 1940      \n",
      " frequency | 1         \n",
      " pages     | 1         \n",
      " books     | 1         \n",
      "-RECORD 9--------------\n",
      " token     | inGermany \n",
      " year      | 1942      \n",
      " frequency | 2         \n",
      " pages     | 2         \n",
      " books     | 2         \n",
      "only showing top 10 rows"
     ]
    }
   ],
   "source": [
    "# Inspect 10 rows\n",
    "\n",
    "ngram_df.show(10, truncate=False, vertical=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb6f0b05",
   "metadata": {},
   "source": [
    "Based on the schema and the above sample, we infer the data format (1-gram Ngrams, per year).\n",
    "\n",
    "Each row represents a single token ``token`` in a specific year ``year``. The other columns are:\n",
    "\n",
    "``frequency``: total occurrences of the token that year.\n",
    "\n",
    "``pages``: number of distinct pages containing the token that year.\n",
    "\n",
    "``books``: number of distinct books containing the token that year.\n",
    "\n",
    "From the first previewed rows, the example token is inGermany, appearing sparsely between 1927–1942 with small counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3c73c1a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|approx_unique_tokens|\n",
      "+--------------------+\n",
      "|             3051522|\n",
      "+--------------------+"
     ]
    }
   ],
   "source": [
    "# Determine approximate count of distinct tokens\n",
    "\n",
    "ngram_df.select(F.approx_count_distinct(\"token\").alias(\"approx_unique_tokens\")).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bfb65c2",
   "metadata": {},
   "source": [
    "The above function approximates there are 3,051,522 unique tokens in our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0bc17722",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+---------+-----+-----+\n",
      "|token|year|frequency|pages|books|\n",
      "+-----+----+---------+-----+-----+\n",
      "|0    |432 |432      |432  |432  |\n",
      "+-----+----+---------+-----+-----+"
     ]
    }
   ],
   "source": [
    "# Check null count for each column\n",
    "\n",
    "nulls = ngram_df.select([\n",
    "    F.sum(F.col(c).isNull().cast(\"int\")).alias(c) for c in ngram_df.columns\n",
    "])\n",
    "nulls.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00095b33",
   "metadata": {},
   "source": [
    "As expected, there are no null ``token`` records. There are 432 null entries for all the other columns, likely the same rows. Given the size of the dataset, this amount of missing values is clearly negligible. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "23b9df04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+--------+--------+---------+---------+---------+---------+\n",
      "|min_year|max_year|min_freq|max_freq|min_pages|max_pages|min_books|max_books|\n",
      "+--------+--------+--------+--------+---------+---------+---------+---------+\n",
      "|    1520|    2008|       1|43571378|        1|  2035864|        1|     6174|\n",
      "+--------+--------+--------+--------+---------+---------+---------+---------+"
     ]
    }
   ],
   "source": [
    "# Check ranges for each numeric column\n",
    "\n",
    "ngram_df.agg(\n",
    "    F.min(\"year\").alias(\"min_year\"), F.max(\"year\").alias(\"max_year\"),\n",
    "    F.min(\"frequency\").alias(\"min_freq\"), F.max(\"frequency\").alias(\"max_freq\"),\n",
    "    F.min(\"pages\").alias(\"min_pages\"), F.max(\"pages\").alias(\"max_pages\"),\n",
    "    F.min(\"books\").alias(\"min_books\"), F.max(\"books\").alias(\"max_books\")\n",
    ").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4356186",
   "metadata": {},
   "source": [
    "The data span 1520–2008 by year (the earliest date predates the referenced 1800s coverage, so we may later restrict to ≥ 1800 for consistency). Frequency ranges from 1 to 43,571,378, with pages up to 2,035,864 and books up to 6,174, reflecting heavy usage of popular tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b078a64",
   "metadata": {},
   "source": [
    "### Filter data and describe new dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4fff03e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Filter for token == \"data\" using Spark SQL\n",
    "\n",
    "ngram_df.createOrReplaceTempView(\"ngrams\")\n",
    "filtered_df = spark.sql(\"\"\"\n",
    "  SELECT token, year, frequency, pages, books\n",
    "  FROM ngrams\n",
    "  WHERE token = 'data'\n",
    "  ORDER BY year\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0c6f11d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of rows is 316. The number of columns is 5."
     ]
    }
   ],
   "source": [
    "# Count the number of rows and columns\n",
    "\n",
    "rows_filtered = filtered_df.count()\n",
    "cols_filtered = len(filtered_df.columns)\n",
    "\n",
    "\n",
    "print(f\"The number of rows is {rows_filtered}. The number of columns is {cols_filtered}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3680faa3",
   "metadata": {},
   "source": [
    "After filtering to only records corresponding to the token \"data,\" we have restricted to only 316 rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "af1df048",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0---------\n",
      " token     | data \n",
      " year      | 1584 \n",
      " frequency | 16   \n",
      " pages     | 14   \n",
      " books     | 1    \n",
      "-RECORD 1---------\n",
      " token     | data \n",
      " year      | 1614 \n",
      " frequency | 3    \n",
      " pages     | 2    \n",
      " books     | 1    \n",
      "-RECORD 2---------\n",
      " token     | data \n",
      " year      | 1627 \n",
      " frequency | 1    \n",
      " pages     | 1    \n",
      " books     | 1    \n",
      "-RECORD 3---------\n",
      " token     | data \n",
      " year      | 1631 \n",
      " frequency | 22   \n",
      " pages     | 18   \n",
      " books     | 1    \n",
      "-RECORD 4---------\n",
      " token     | data \n",
      " year      | 1637 \n",
      " frequency | 1    \n",
      " pages     | 1    \n",
      " books     | 1    \n",
      "-RECORD 5---------\n",
      " token     | data \n",
      " year      | 1638 \n",
      " frequency | 2    \n",
      " pages     | 2    \n",
      " books     | 1    \n",
      "-RECORD 6---------\n",
      " token     | data \n",
      " year      | 1640 \n",
      " frequency | 1    \n",
      " pages     | 1    \n",
      " books     | 1    \n",
      "-RECORD 7---------\n",
      " token     | data \n",
      " year      | 1642 \n",
      " frequency | 1    \n",
      " pages     | 1    \n",
      " books     | 1    \n",
      "-RECORD 8---------\n",
      " token     | data \n",
      " year      | 1644 \n",
      " frequency | 4    \n",
      " pages     | 4    \n",
      " books     | 1    \n",
      "-RECORD 9---------\n",
      " token     | data \n",
      " year      | 1647 \n",
      " frequency | 1    \n",
      " pages     | 1    \n",
      " books     | 1    \n",
      "only showing top 10 rows"
     ]
    }
   ],
   "source": [
    "# Inspect 10 rows\n",
    "\n",
    "filtered_df.show(10, truncate=False, vertical=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c369595",
   "metadata": {},
   "source": [
    "A sample of 10 records in our new dataset shows some early mentions of the word \"data\" going back as far as the 1500s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a751d246",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distinct tokens: 1"
     ]
    }
   ],
   "source": [
    "# Confirm the only token is now \"data\"\n",
    "\n",
    "uniq = filtered_df.select(\"token\").distinct()\n",
    "n_uniq = uniq.count()\n",
    "print(\"Distinct tokens:\", n_uniq)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d47aa133",
   "metadata": {},
   "source": [
    "Since our ``token`` column no longer distinguishes records, we drop the column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4fc5297b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns after drop: ['year', 'frequency', 'pages', 'books']"
     ]
    }
   ],
   "source": [
    "# Drop the token column\n",
    "\n",
    "filtered_df = filtered_df.drop(\"token\")\n",
    "\n",
    "print(\"Columns after drop:\", filtered_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "03bd4d14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------+-----+-----+\n",
      "|year|frequency|pages|books|\n",
      "+----+---------+-----+-----+\n",
      "|0   |0        |0    |0    |\n",
      "+----+---------+-----+-----+"
     ]
    }
   ],
   "source": [
    "# Confirm there are no nulls\n",
    "\n",
    "nulls_filtered = filtered_df.select([\n",
    "    F.sum(F.col(c).isNull().cast(\"int\")).alias(c) for c in filtered_df.columns\n",
    "])\n",
    "nulls_filtered.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "78b00ada",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------+---------+---------+\n",
      "|min_year|min_frequency|min_pages|min_books|\n",
      "+--------+-------------+---------+---------+\n",
      "|1584    |1            |1        |1        |\n",
      "+--------+-------------+---------+---------+\n",
      "\n",
      "+--------+-------------+---------+---------+\n",
      "|max_year|max_frequency|max_pages|max_books|\n",
      "+--------+-------------+---------+---------+\n",
      "|2008    |254561       |122472   |4372     |\n",
      "+--------+-------------+---------+---------+"
     ]
    }
   ],
   "source": [
    "# Determine numeric ranges for all columns\n",
    "\n",
    "mins = filtered_df.agg(*(F.min(c).alias(f\"min_{c}\") for c in filtered_df.columns))\n",
    "maxs = filtered_df.agg(*(F.max(c).alias(f\"max_{c}\") for c in filtered_df.columns))\n",
    "mins.show(truncate=False)\n",
    "maxs.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb3f6ab3",
   "metadata": {},
   "source": [
    "Checking the value ranges for each column, the years span from 1584 to 2008. The minimum frequency of the word \"data\" appearing is 1, while the maximum is 254,561. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a450b24",
   "metadata": {},
   "source": [
    "### Write filtered data back into HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4b9db66b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote CSV directory: hdfs:///user/hadoop/outputs/data_token_csv"
     ]
    }
   ],
   "source": [
    "# Use .write.csv() to write filtered data back into HDFS\n",
    "\n",
    "out_dir = \"hdfs:///user/hadoop/outputs/data_token_csv\"\n",
    "\n",
    "\n",
    "(filtered_df\n",
    "    .orderBy(\"year\")         # Sort by year\n",
    "    .coalesce(1)             # Ensure single file\n",
    "    .write\n",
    "    .csv(out_dir, mode=\"overwrite\", header=True)) # Specify header = True\n",
    "\n",
    "print(\"Wrote CSV directory:\", out_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cb914f43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows written: 316\n",
      "root\n",
      " |-- year: integer (nullable = true)\n",
      " |-- frequency: integer (nullable = true)\n",
      " |-- pages: integer (nullable = true)\n",
      " |-- books: integer (nullable = true)\n",
      "\n",
      "+----+---------+-----+-----+\n",
      "|year|frequency|pages|books|\n",
      "+----+---------+-----+-----+\n",
      "|1584|16       |14   |1    |\n",
      "|1614|3        |2    |1    |\n",
      "|1627|1        |1    |1    |\n",
      "|1631|22       |18   |1    |\n",
      "|1637|1        |1    |1    |\n",
      "+----+---------+-----+-----+\n",
      "only showing top 5 rows"
     ]
    }
   ],
   "source": [
    "# Read filtered data back to confirm the last step executed correctly\n",
    "\n",
    "written_df = spark.read.csv(out_dir, header=True, inferSchema=True)\n",
    "print(\"Rows written:\", written_df.count())\n",
    "written_df.printSchema()\n",
    "written_df.show(5, truncate=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
